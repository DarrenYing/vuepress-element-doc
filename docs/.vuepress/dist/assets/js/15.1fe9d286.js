(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{592:function(a,t,s){"use strict";s.r(t);var e=s(38),n=Object(e.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("h1",{attrs:{id:"pytorch分布式训练简介"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pytorch分布式训练简介"}},[a._v("#")]),a._v(" Pytorch分布式训练简介")]),a._v(" "),s("h2",{attrs:{id:"dataparallel"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dataparallel"}},[a._v("#")]),a._v(" DataParallel")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("class")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("torch")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("DataParallel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" device_ids"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" output_device"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("blockquote",[s("ul",[s("li",[s("p",[a._v("自动分割数据，将相同的模型复制到所有GPU，每个GPU消耗输入数据的不同分区")])]),a._v(" "),s("li",[s("p",[a._v("通过单进程、多线程方式并行训练，受限于python的GIL")])]),a._v(" "),s("li",[s("p",[a._v("只支持数据并行，性能不佳")])]),a._v(" "),s("li",[s("p",[a._v("网络在"),s("strong",[a._v("前向传播")]),a._v("的时候会将model从主卡(默认是逻辑0卡)复制一份到所有的device上,input_data会在batch这个维度被分组后upload到不同的device上计算。在"),s("strong",[a._v("反向传播")]),a._v("时，每个卡上的梯度会汇总到主卡上,求得梯度的均值后,再用反向传播更新单个GPU上的模型参数，最后将更新后的模型参数复制到剩余指定的GPU中进行下一轮的前向传播,以此来实现并行。")])])])]),a._v(" "),s("h2",{attrs:{id:"distributeddataparallel"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#distributeddataparallel"}},[a._v("#")]),a._v(" DistributedDataParallel")]),a._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[a._v("CLASS "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("torch")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("nn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("parallel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("DistributedDataParallel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" device_ids"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n    output_device"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" dim"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" broadcast_buffers"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" process_group"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n    bucket_cap_mb"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("25")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" find_unused_parameters"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" check_reduction"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" \n    gradient_as_bucket_view"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("blockquote",[s("ul",[s("li",[s("p",[a._v("支持数据并行和模型并行")])]),a._v(" "),s("li",[s("p",[a._v("多进程并行训练，不受限于python的GIL")])]),a._v(" "),s("li",[s("p",[a._v("DDP在各进程梯度计算完成之后,各进程需要将梯度进行汇总平均,然后再由 "),s("code",[a._v("rank=0")]),a._v(" 的进程,将其 "),s("code",[a._v("broadcast")]),a._v(" 到所有进程后,各进程用该梯度来独立的更新参数，相较于DP, DDP传输的数据量更少,因此速度更快,效率更高。")])]),a._v(" "),s("li",[s("p",[a._v("效率较高，更推荐使用。CPU训练使用GLOO，GPU训练使用NCCL。")])])])])])}),[],!1,null,null,null);t.default=n.exports}}]);