<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>常见问题记录和解答 | Pytorch Distributed Training Tutorial</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="使用Pytorch分布式训练的示例代码">
    
    <link rel="preload" href="/assets/css/0.styles.262ade0c.css" as="style"><link rel="preload" href="/assets/js/app.10116606.js" as="script"><link rel="preload" href="/assets/js/2.074008b5.js" as="script"><link rel="preload" href="/assets/js/14.25cd57c8.js" as="script"><link rel="prefetch" href="/assets/js/10.83cfe51a.js"><link rel="prefetch" href="/assets/js/11.d0af463f.js"><link rel="prefetch" href="/assets/js/12.401d4db2.js"><link rel="prefetch" href="/assets/js/13.481f27e7.js"><link rel="prefetch" href="/assets/js/15.1fe9d286.js"><link rel="prefetch" href="/assets/js/16.1128fa1c.js"><link rel="prefetch" href="/assets/js/3.dfffe9d2.js"><link rel="prefetch" href="/assets/js/4.73c9d461.js"><link rel="prefetch" href="/assets/js/5.0857751a.js"><link rel="prefetch" href="/assets/js/6.ca83e7e8.js"><link rel="prefetch" href="/assets/js/7.29bf9295.js"><link rel="prefetch" href="/assets/js/8.5bc1f666.js"><link rel="prefetch" href="/assets/js/9.9f74d9b1.js">
    <link rel="stylesheet" href="/assets/css/0.styles.262ade0c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Pytorch Distributed Training Tutorial</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/comps/" class="nav-link">
  组件
</a></div><div class="nav-item"><a href="/pytorch/" class="nav-link router-link-active">
  pytorch
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/comps/" class="nav-link">
  组件
</a></div><div class="nav-item"><a href="/pytorch/" class="nav-link router-link-active">
  pytorch
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>分布式训练</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pytorch/" aria-current="page" class="sidebar-link">Pytorch分布式训练简介</a></li><li><a href="/pytorch/Use-DistributedDataParallel.html" class="sidebar-link">使用Pytorch进行分布式训练</a></li><li><a href="/pytorch/Issue-Record.html" aria-current="page" class="active sidebar-link">常见问题记录和解答</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/pytorch/Issue-Record.html#_1-单机多卡训练时-多卡使用率不均衡" class="sidebar-link">1.单机多卡训练时，多卡使用率不均衡</a></li><li class="sidebar-sub-header"><a href="/pytorch/Issue-Record.html#_2-数据集的随机数种子固定" class="sidebar-link">2.数据集的随机数种子固定</a></li><li class="sidebar-sub-header"><a href="/pytorch/Issue-Record.html#_3-多卡控制台输出信息重复" class="sidebar-link">3.多卡控制台输出信息重复</a></li><li class="sidebar-sub-header"><a href="/pytorch/Issue-Record.html#_4-多卡的训练精度不如单卡" class="sidebar-link">4.多卡的训练精度不如单卡</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="常见问题记录和解答"><a href="#常见问题记录和解答" class="header-anchor">#</a> 常见问题记录和解答</h1> <h2 id="_1-单机多卡训练时-多卡使用率不均衡"><a href="#_1-单机多卡训练时-多卡使用率不均衡" class="header-anchor">#</a> 1.单机多卡训练时，多卡使用率不均衡</h2> <blockquote><p>可能是由于num_workers过小，CPU读取数据慢，读的数据只够一张卡训练，可以尝试调大num_workers</p> <p>推荐将num_workers数量设置为GPU数量的4倍，开启pin_memory</p></blockquote> <h2 id="_2-数据集的随机数种子固定"><a href="#_2-数据集的随机数种子固定" class="header-anchor">#</a> 2.数据集的随机数种子固定</h2> <blockquote><p>在每个epoch重置data_sampler的随机数种子，<code>train_loader.sampler.set_epoch(epoch)</code></p></blockquote> <h2 id="_3-多卡控制台输出信息重复"><a href="#_3-多卡控制台输出信息重复" class="header-anchor">#</a> 3.多卡控制台输出信息重复</h2> <blockquote><p>推荐使用logging代替print进行信息打印，并给不同进程设置不同的输出级别，只在0号进程（主进程）保留全部输出。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> logging
<span class="token comment"># 给主要进程（rank=0）设置低输出等级，给其他进程设置高输出等级。</span>
logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO <span class="token keyword">if</span> rank <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">else</span> logging<span class="token punctuation">.</span>WARN<span class="token punctuation">)</span>
<span class="token comment"># 普通log，只会打印一次。</span>
logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">&quot;This is an ordinary log.&quot;</span><span class="token punctuation">)</span>
<span class="token comment"># 危险的warning、error，无论在哪个进程，都会被打印出来，从而方便debug。</span>
logging<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string">&quot;This is a fatal log!&quot;</span><span class="token punctuation">)</span>
</code></pre></div></blockquote> <h2 id="_4-多卡的训练精度不如单卡"><a href="#_4-多卡的训练精度不如单卡" class="header-anchor">#</a> 4.多卡的训练精度不如单卡</h2> <blockquote><p>如果单机卡数为n，需要修改batch_size为单卡时的1/n，如果增大batch_size,需要将learning rate也相应增大</p> <p>如果model中有BatchNorm层，推荐开启SyncBN</p> <p><code>model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)</code></p></blockquote></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/pytorch/Use-DistributedDataParallel.html" class="prev">
        使用Pytorch进行分布式训练
      </a></span> <!----></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.10116606.js" defer></script><script src="/assets/js/2.074008b5.js" defer></script><script src="/assets/js/14.25cd57c8.js" defer></script>
  </body>
</html>
