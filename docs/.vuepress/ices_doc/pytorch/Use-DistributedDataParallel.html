<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>使用Pytorch进行分布式训练 | ICES Distributed Training System Tutorial</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="使用ICES网盘系统进行分布式训练">
    
    <link rel="preload" href="/docs/assets/css/0.styles.5d9619c0.css" as="style"><link rel="preload" href="/docs/assets/js/app.d2de212f.js" as="script"><link rel="preload" href="/docs/assets/js/2.4bd5cfe5.js" as="script"><link rel="preload" href="/docs/assets/js/11.afa7b2c1.js" as="script"><link rel="prefetch" href="/docs/assets/js/10.c451b574.js"><link rel="prefetch" href="/docs/assets/js/12.6ce4a108.js"><link rel="prefetch" href="/docs/assets/js/13.582c97ff.js"><link rel="prefetch" href="/docs/assets/js/14.950dbfe3.js"><link rel="prefetch" href="/docs/assets/js/15.1928fc36.js"><link rel="prefetch" href="/docs/assets/js/3.c724ef06.js"><link rel="prefetch" href="/docs/assets/js/4.0d7d8041.js"><link rel="prefetch" href="/docs/assets/js/5.8b141ec4.js"><link rel="prefetch" href="/docs/assets/js/6.654a3df5.js"><link rel="prefetch" href="/docs/assets/js/7.b5fa177a.js"><link rel="prefetch" href="/docs/assets/js/8.1a34521d.js"><link rel="prefetch" href="/docs/assets/js/9.510aac33.js">
    <link rel="stylesheet" href="/docs/assets/css/0.styles.5d9619c0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/docs/" class="home-link router-link-active"><!----> <span class="site-name">ICES Distributed Training System Tutorial</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/docs/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/docs/storage/" class="nav-link">
  ICES网盘
</a></div><div class="nav-item"><a href="/docs/pytorch/" class="nav-link router-link-active">
  pytorch
</a></div><div class="nav-item"><a href="https://github.com/DarrenYing/vuepress-element-doc" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/docs/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/docs/storage/" class="nav-link">
  ICES网盘
</a></div><div class="nav-item"><a href="/docs/pytorch/" class="nav-link router-link-active">
  pytorch
</a></div><div class="nav-item"><a href="https://github.com/DarrenYing/vuepress-element-doc" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>分布式训练</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/pytorch/" aria-current="page" class="sidebar-link">Pytorch分布式训练简介</a></li><li><a href="/docs/pytorch/Use-DistributedDataParallel.html" aria-current="page" class="active sidebar-link">使用Pytorch进行分布式训练</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/pytorch/Use-DistributedDataParallel.html#从单卡到多卡" class="sidebar-link">从单卡到多卡</a></li><li class="sidebar-sub-header"><a href="/docs/pytorch/Use-DistributedDataParallel.html#整体代码结构" class="sidebar-link">整体代码结构</a></li></ul></li><li><a href="/docs/pytorch/Issue-Record.html" class="sidebar-link">常见问题记录和解答</a></li><li><a href="/docs/pytorch/MultiNode-Distributed.html" class="sidebar-link">多机多卡训练配置</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="使用pytorch进行分布式训练"><a href="#使用pytorch进行分布式训练" class="header-anchor">#</a> 使用Pytorch进行分布式训练</h1> <h2 id="从单卡到多卡"><a href="#从单卡到多卡" class="header-anchor">#</a> 从单卡到多卡</h2> <h3 id="导入包"><a href="#导入包" class="header-anchor">#</a> 导入包</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> os
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> DDP
</code></pre></div><h3 id="初始化进程组"><a href="#初始化进程组" class="header-anchor">#</a> 初始化进程组</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># initialize the process group</span>
rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;LOCAL_RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
world_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;WORLD_SIZE&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>rank <span class="token operator">%</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">&quot;nccl&quot;</span><span class="token punctuation">)</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">,</span> local_rank<span class="token punctuation">)</span>
</code></pre></div><h3 id="数据集"><a href="#数据集" class="header-anchor">#</a> 数据集</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># define your dataset</span>
train_set <span class="token operator">=</span> your_training_dataset<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'train_path'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>some transforms<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
train_sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>
    train_set<span class="token punctuation">,</span>
    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>
    train_set<span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span>NUM_WORKERS<span class="token punctuation">,</span>
    pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    sampler<span class="token operator">=</span>train_sampler<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

test_set <span class="token operator">=</span> your_testing_dataset<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'test_path'</span><span class="token punctuation">)</span>
test_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>
    test_set<span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>
    shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span>NUM_WORKERS<span class="token punctuation">)</span>
</code></pre></div><h3 id="模型"><a href="#模型" class="header-anchor">#</a> 模型</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># define your model</span>
model <span class="token operator">=</span> your_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment"># if open SyncBN</span>
<span class="token keyword">if</span> args<span class="token punctuation">.</span>syncBN<span class="token punctuation">:</span>
    model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">.</span>convert_sync_batchnorm<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token comment"># use DDP</span>
model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">)</span>

<span class="token comment"># define your optimizer and loss function</span>
criterion <span class="token operator">=</span> your_loss_function<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> your_optimizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="训练"><a href="#训练" class="header-anchor">#</a> 训练</h3> <div class="language-python extra-class"><pre class="language-python"><code>model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># reset random seed every epoch</span>
    train_loader<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
    
    <span class="token comment"># normal training procedure</span>
    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
        <span class="token comment"># some other operations</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
        <span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> other conditions<span class="token punctuation">:</span>
            log_something<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># only save model on rank-0</span>
    <span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> epoch <span class="token operator">%</span> args<span class="token punctuation">.</span>save_epochs <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CHECKPOINT_PATH<span class="token punctuation">)</span>
</code></pre></div><h3 id="测试"><a href="#测试" class="header-anchor">#</a> 测试</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> data <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>
        <span class="token comment"># some test operations</span>
        <span class="token comment"># do not need to change your original test code</span>
</code></pre></div><h3 id="启动命令"><a href="#启动命令" class="header-anchor">#</a> 启动命令</h3> <div class="language-shell extra-class"><pre class="language-shell"><code><span class="token comment"># 单机多卡的两种启动命令</span>
python -m torch.distributed.launch --nproc_per_node<span class="token operator">=</span><span class="token number">2</span> --nnodes<span class="token operator">=</span><span class="token number">1</span> main.py <span class="token punctuation">(</span>--arg1 <span class="token punctuation">..</span>. train script args<span class="token punctuation">..</span>.<span class="token punctuation">)</span>
torchrun --standlone --nproc_per_node<span class="token operator">=</span><span class="token number">2</span> main.py <span class="token punctuation">(</span>--arg1 <span class="token punctuation">..</span>. train script args<span class="token punctuation">..</span>.<span class="token punctuation">)</span>
</code></pre></div><h2 id="整体代码结构"><a href="#整体代码结构" class="header-anchor">#</a> 整体代码结构</h2> <h3 id="依赖导入"><a href="#依赖导入" class="header-anchor">#</a> 依赖导入</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token comment"># ...</span>
</code></pre></div><h3 id="定义参数"><a href="#定义参数" class="header-anchor">#</a> 定义参数</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">add_argument</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">'Template'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># multiprocessing settings</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--local_rank'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'node rank for distributed training'</span><span class="token punctuation">)</span>
    <span class="token comment"># other arguments</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-b'</span><span class="token punctuation">,</span>
                     <span class="token string">'--batch_size'</span><span class="token punctuation">,</span>
                     default<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>
                     <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span>
                     <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'mini-batch size (default: 32)'</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-e'</span><span class="token punctuation">,</span>
                     <span class="token string">'--epochs'</span><span class="token punctuation">,</span>
                     default<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
                     <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span>
                     <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'number of total epochs (default: 10)'</span><span class="token punctuation">)</span>
    <span class="token comment"># ...</span>
    
    args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> args
</code></pre></div><h3 id="定义数据集"><a href="#定义数据集" class="header-anchor">#</a> 定义数据集</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">make_data_loader</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_set <span class="token operator">=</span> your_dataset<span class="token punctuation">(</span>args<span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span>
    val_set <span class="token operator">=</span> your_dataset<span class="token punctuation">(</span>args<span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">'val'</span><span class="token punctuation">)</span>
    test_set <span class="token operator">=</span> your_dataset<span class="token punctuation">(</span>args<span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">'test'</span><span class="token punctuation">)</span>
    <span class="token comment"># use distributed sampler</span>
    train_sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>train_set<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_set<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> sampler<span class="token operator">=</span>train_sampler<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    <span class="token comment"># do not need distributed wrapped</span>
    val_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>val_set<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>test_batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_set<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>test_batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    <span class="token keyword">return</span> train_loader<span class="token punctuation">,</span> val_loader<span class="token punctuation">,</span> test_loader
</code></pre></div><h3 id="定义trainer"><a href="#定义trainer" class="header-anchor">#</a> 定义Trainer</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Trainer</span><span class="token punctuation">(</span>obejct<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># initialize the environment</span>
        rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;LOCAL_RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>rank <span class="token operator">%</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">&quot;nccl&quot;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda&quot;</span><span class="token punctuation">,</span> local_rank<span class="token punctuation">)</span>
        
        <span class="token comment"># define dataloader</span>
        kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'num_workers'</span><span class="token punctuation">:</span> args<span class="token punctuation">.</span>workers<span class="token punctuation">,</span> <span class="token string">'pin_memory'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span>
        self<span class="token punctuation">.</span>train_loader<span class="token punctuation">,</span> self<span class="token punctuation">.</span>val_loader<span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_loader <span class="token operator">=</span> make_data_loader<span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        
        <span class="token comment"># define model</span>
        model <span class="token operator">=</span> your_model<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
        model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        <span class="token comment"># if open SyncBN</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>syncBN<span class="token punctuation">:</span>
            model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">.</span>convert_sync_batchnorm<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
        <span class="token comment"># wrap model with DDP</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">)</span>
        <span class="token comment"># resume checkpoint</span>
        self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>args<span class="token punctuation">.</span>checkpoint_path<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment"># define other configs</span>
        self<span class="token punctuation">.</span>criterion <span class="token operator">=</span> your_loss_function<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>optimizer <span class="token operator">=</span> your_optimizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lr_scheduler <span class="token operator">=</span> your_lr_scheduler<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># your training code</span>
        <span class="token comment"># ...</span>
    
    <span class="token keyword">def</span> <span class="token function">validate</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># your validation code</span>
        <span class="token comment"># ...</span>
        
    <span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># your testing code</span>
            <span class="token comment"># ...    </span>
        
</code></pre></div><h3 id="主函数"><a href="#主函数" class="header-anchor">#</a> 主函数</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    args <span class="token operator">=</span> add_argument<span class="token punctuation">(</span><span class="token punctuation">)</span>
    trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>start_epoch<span class="token punctuation">,</span> args<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> trainer<span class="token punctuation">.</span>args<span class="token punctuation">.</span>no_val <span class="token keyword">and</span> epoch <span class="token operator">%</span> args<span class="token punctuation">.</span>eval_interval <span class="token operator">==</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>eval_interval <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            trainer<span class="token punctuation">.</span>validate<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
    trainer<span class="token punctuation">.</span>test<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h3 id="入口函数"><a href="#入口函数" class="header-anchor">#</a> 入口函数</h3> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">&quot;__main__&quot;</span><span class="token punctuation">:</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/docs/pytorch/" class="prev router-link-active">
        Pytorch分布式训练简介
      </a></span> <span class="next"><a href="/docs/pytorch/Issue-Record.html">
        常见问题记录和解答
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/docs/assets/js/app.d2de212f.js" defer></script><script src="/docs/assets/js/2.4bd5cfe5.js" defer></script><script src="/docs/assets/js/11.afa7b2c1.js" defer></script>
  </body>
</html>
