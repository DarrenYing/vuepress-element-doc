<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Pytorch分布式训练简介 | ICES Distributed Training System Tutorial</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="使用ICES网盘系统进行分布式训练">
    
    <link rel="preload" href="/docs/assets/css/0.styles.5d9619c0.css" as="style"><link rel="preload" href="/docs/assets/js/app.d2de212f.js" as="script"><link rel="preload" href="/docs/assets/js/2.4bd5cfe5.js" as="script"><link rel="preload" href="/docs/assets/js/10.c451b574.js" as="script"><link rel="prefetch" href="/docs/assets/js/11.afa7b2c1.js"><link rel="prefetch" href="/docs/assets/js/12.6ce4a108.js"><link rel="prefetch" href="/docs/assets/js/13.582c97ff.js"><link rel="prefetch" href="/docs/assets/js/14.950dbfe3.js"><link rel="prefetch" href="/docs/assets/js/15.1928fc36.js"><link rel="prefetch" href="/docs/assets/js/3.c724ef06.js"><link rel="prefetch" href="/docs/assets/js/4.0d7d8041.js"><link rel="prefetch" href="/docs/assets/js/5.8b141ec4.js"><link rel="prefetch" href="/docs/assets/js/6.654a3df5.js"><link rel="prefetch" href="/docs/assets/js/7.b5fa177a.js"><link rel="prefetch" href="/docs/assets/js/8.1a34521d.js"><link rel="prefetch" href="/docs/assets/js/9.510aac33.js">
    <link rel="stylesheet" href="/docs/assets/css/0.styles.5d9619c0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/docs/" class="home-link router-link-active"><!----> <span class="site-name">ICES Distributed Training System Tutorial</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/docs/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/docs/storage/" class="nav-link">
  ICES网盘
</a></div><div class="nav-item"><a href="/docs/pytorch/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  pytorch
</a></div><div class="nav-item"><a href="https://github.com/DarrenYing/vuepress-element-doc" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/docs/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/docs/storage/" class="nav-link">
  ICES网盘
</a></div><div class="nav-item"><a href="/docs/pytorch/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  pytorch
</a></div><div class="nav-item"><a href="https://github.com/DarrenYing/vuepress-element-doc" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>分布式训练</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/pytorch/" aria-current="page" class="active sidebar-link">Pytorch分布式训练简介</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/docs/pytorch/#dataparallel" class="sidebar-link">DataParallel</a></li><li class="sidebar-sub-header"><a href="/docs/pytorch/#distributeddataparallel" class="sidebar-link">DistributedDataParallel</a></li></ul></li><li><a href="/docs/pytorch/Use-DistributedDataParallel.html" class="sidebar-link">使用Pytorch进行分布式训练</a></li><li><a href="/docs/pytorch/Issue-Record.html" class="sidebar-link">常见问题记录和解答</a></li><li><a href="/docs/pytorch/MultiNode-Distributed.html" class="sidebar-link">多机多卡训练配置</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="pytorch分布式训练简介"><a href="#pytorch分布式训练简介" class="header-anchor">#</a> Pytorch分布式训练简介</h1> <h2 id="dataparallel"><a href="#dataparallel" class="header-anchor">#</a> DataParallel</h2> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">torch</span><span class="token punctuation">.</span>nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>module<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre></div><blockquote><ul><li><p>自动分割数据，将相同的模型复制到所有GPU，每个GPU消耗输入数据的不同分区</p></li> <li><p>通过单进程、多线程方式并行训练，受限于python的GIL</p></li> <li><p>只支持数据并行，性能不佳</p></li> <li><p>网络在<strong>前向传播</strong>的时候会将model从主卡(默认是逻辑0卡)复制一份到所有的device上,input_data会在batch这个维度被分组后upload到不同的device上计算。在<strong>反向传播</strong>时，每个卡上的梯度会汇总到主卡上,求得梯度的均值后,再用反向传播更新单个GPU上的模型参数，最后将更新后的模型参数复制到剩余指定的GPU中进行下一轮的前向传播,以此来实现并行。</p></li></ul></blockquote> <h2 id="distributeddataparallel"><a href="#distributeddataparallel" class="header-anchor">#</a> DistributedDataParallel</h2> <div class="language-python extra-class"><pre class="language-python"><code>CLASS <span class="token class-name">torch</span><span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>module<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
    output_device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> broadcast_buffers<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> process_group<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
    bucket_cap_mb<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> find_unused_parameters<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> check_reduction<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
    gradient_as_bucket_view<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre></div><blockquote><ul><li><p>支持数据并行和模型并行</p></li> <li><p>多进程并行训练，不受限于python的GIL</p></li> <li><p>DDP在各进程梯度计算完成之后,各进程需要将梯度进行汇总平均,然后再由 <code>rank=0</code> 的进程,将其 <code>broadcast</code> 到所有进程后,各进程用该梯度来独立的更新参数，相较于DP, DDP传输的数据量更少,因此速度更快,效率更高。</p></li> <li><p>效率较高，更推荐使用。CPU训练使用GLOO，GPU训练使用NCCL。</p></li></ul></blockquote></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><!----> <span class="next"><a href="/docs/pytorch/Use-DistributedDataParallel.html">
        使用Pytorch进行分布式训练
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/docs/assets/js/app.d2de212f.js" defer></script><script src="/docs/assets/js/2.4bd5cfe5.js" defer></script><script src="/docs/assets/js/10.c451b574.js" defer></script>
  </body>
</html>
